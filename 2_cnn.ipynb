{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Download Dataset and Library"
      ],
      "metadata": {
        "id": "e3XgBGSSuRrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install pycocotools\n",
        "\n",
        "!wget http://images.cocodataset.org/zips/train2017.zip\n",
        "!unzip train2017.zip\n",
        "\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip annotations_trainval2017.zip"
      ],
      "metadata": {
        "id": "C5ByglyRtvRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#import Library"
      ],
      "metadata": {
        "id": "gQz77-CiuFab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torchvision import models, datasets, tv_tensors\n",
        "from torchvision.transforms import v2\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "TSfTAzFstyB5",
        "outputId": "0d8c8eba-6db9-4327-a5a5-f6ace9e79b99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7edfc83d5770>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset preparation"
      ],
      "metadata": {
        "id": "hhPZE-XpuXfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = '/content/train2017'\n",
        "annFile = '/content/annotations/instances_train2017.json'\n",
        "dataset = datasets.CocoDetection(root, annFile)\n",
        "sample = dataset[0]\n",
        "img, target = sample\n",
        "print(f\"{type(img) = }\\n{type(target) = }\\n{type(target[0]) = }\\n{target[0].keys() = }\")"
      ],
      "metadata": {
        "id": "bhcB8yQiumt-",
        "outputId": "e19cea3e-2572-492f-f239-0f445cb5148d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=23.29s)\n",
            "creating index...\n",
            "index created!\n",
            "type(img) = <class 'PIL.Image.Image'>\n",
            "type(target) = <class 'list'>\n",
            "type(target[0]) = <class 'dict'>\n",
            "target[0].keys() = dict_keys(['segmentation', 'area', 'iscrowd', 'image_id', 'bbox', 'category_id', 'id'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.wrap_dataset_for_transforms_v2(dataset, target_keys=(\"boxes\", \"labels\", \"masks\"))\n",
        "sample = dataset[0]\n",
        "img, target = sample\n",
        "print(f\"{type(img) = }\\n{type(target) = }\\n{target.keys() = }\")\n",
        "print(f\"{type(target['boxes']) = }\\n{type(target['labels']) = }\\n{type(target['masks']) = }\")"
      ],
      "metadata": {
        "id": "D5_ie7Yzuxc0",
        "outputId": "16d55a1f-6e39-470f-f4b1-8450b61a11d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(img) = <class 'PIL.Image.Image'>\n",
            "type(target) = <class 'dict'>\n",
            "target.keys() = dict_keys(['boxes', 'masks', 'labels'])\n",
            "type(target['boxes']) = <class 'torchvision.tv_tensors._bounding_boxes.BoundingBoxes'>\n",
            "type(target['labels']) = <class 'torch.Tensor'>\n",
            "type(target['masks']) = <class 'torchvision.tv_tensors._mask.Mask'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transforms"
      ],
      "metadata": {
        "id": "IHkN4LemwrP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = v2.Compose(\n",
        "    [\n",
        "        v2.ToImage(),\n",
        "        v2.RandomPhotometricDistort(p=1),\n",
        "        v2.RandomZoomOut(fill={tv_tensors.Image: (123, 117, 104), \"others\": 0}),\n",
        "        v2.RandomIoUCrop(),\n",
        "        v2.RandomHorizontalFlip(p=1),\n",
        "        v2.SanitizeBoundingBoxes(),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "    ]\n",
        ")\n",
        "dataset = datasets.CocoDetection(root, annFile, transforms=transforms)\n",
        "dataset = datasets.wrap_dataset_for_transforms_v2(dataset, target_keys=[\"boxes\", \"labels\", \"masks\"])"
      ],
      "metadata": {
        "id": "a5HGumiZwjXo",
        "outputId": "c47f49bf-3614-4593-b201-1d7ba3bbb4fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=22.07s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data loading and training loop"
      ],
      "metadata": {
        "id": "VMxMkoMSxGeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
        ")\n",
        "\n",
        "model = models.get_model(\"maskrcnn_resnet50_fpn_v2\", weights=None, weights_backbone=None).train()\n",
        "\n",
        "for imgs, targets in data_loader:\n",
        "    loss_dict = model(imgs, targets)\n",
        "\n",
        "    print(f\"{[img.shape for img in imgs] = }\")\n",
        "    print(f\"{[type(target) for target in targets] = }\")\n",
        "    for name, loss_val in loss_dict.items():\n",
        "        print(f\"{name:<20}{loss_val:.3f}\")"
      ],
      "metadata": {
        "id": "iu8fcmoBxGrG",
        "outputId": "bd3ab3e5-9033-4d27-cfe9-27688429d1ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[img.shape for img in imgs] = [torch.Size([3, 1776, 2368]), torch.Size([3, 1258, 1890])]\n",
            "[type(target) for target in targets] = [<class 'dict'>, <class 'dict'>]\n",
            "loss_classifier     4.591\n",
            "loss_box_reg        0.068\n",
            "loss_mask           0.796\n",
            "loss_objectness     0.696\n",
            "loss_rpn_box_reg    0.028\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}