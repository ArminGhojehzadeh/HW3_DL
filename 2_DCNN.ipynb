{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install pycocotools"
      ],
      "metadata": {
        "id": "8hrUw7szFmPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/zips/train2017.zip\n",
        "!unzip train2017.zip\n",
        "\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip annotations_trainval2017.zip"
      ],
      "metadata": {
        "id": "gZClH2itFmTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.transforms.functional import resize, to_tensor\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import ToTensor, RandomResizedCrop\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Define the Deformable Convolutional Layer\n",
        "class DeformableConv2dLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super(DeformableConv2dLayer, self).__init__()\n",
        "        self.conv_offset = nn.Conv2d(in_channels, 2 * kernel_size * kernel_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.deform_conv = torchvision.ops.DeformConv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        offset = self.conv_offset(x)\n",
        "        x = self.deform_conv(x, offset)\n",
        "        return x\n",
        "\n",
        "# Define the Deformable Convolutional Neural Network (DCNN) model\n",
        "class DeformableCNNModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(DeformableCNNModel, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            DeformableConv2dLayer(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            DeformableConv2dLayer(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.roi_pooling = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.fc = nn.Linear(128 * 7 * 7, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.roi_pooling(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Load COCO dataset\n",
        "root = '/content/train2017'\n",
        "annFile = '/content/annotations/instances_train2017.json'\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "dataset = CocoDetection(root, annFile, transform=transform)\n",
        "\n",
        "# Randomly sample 0.1% of the indices\n",
        "indices1 = np.random.choice(len(dataset), int(0.001 * len(dataset)), replace=False)\n",
        "\n",
        "# Use only the sampled indices for the dataset\n",
        "subset_dataset = Subset(dataset, indices1)\n",
        "\n",
        "# Use a custom collate function to handle COCO annotations\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)\n",
        "    images = [resize(img, (480, 640)) for img in images]\n",
        "    images = torch.stack(images)\n",
        "\n",
        "    # Handle COCO targets format\n",
        "    new_targets = []\n",
        "    for target in targets:\n",
        "        if len(target) > 0 and 'category_id' in target[0]:  # Check if it's not empty and already in the desired format\n",
        "            new_targets.extend(target)\n",
        "        else:  # Convert from COCO format to the desired format\n",
        "            new_targets.extend([{\"category_id\": t[\"category_id\"]} for t in target])\n",
        "\n",
        "    return images, new_targets\n",
        "\n",
        "data_loader = DataLoader(subset_dataset, batch_size=2, collate_fn=collate_fn)\n",
        "\n",
        "# Instantiate the model\n",
        "num_classes = len(dataset.coco.getCatIds())\n",
        "model = DeformableCNNModel(num_classes=num_classes)\n",
        "num_epochs = 3\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    for images, targets in data_loader:\n",
        "        # Assuming targets is a list of dictionaries with 'category_id' key\n",
        "        target_indices = [target['category_id'] for target in targets]\n",
        "\n",
        "        # Dynamically adjust the number of classes based on the max class index in the batch\n",
        "        num_classes = max(target_indices) + 1\n",
        "\n",
        "        # Ensure the batch size of targets matches the model output\n",
        "        # Adjust the number of output units in the model's final layer\n",
        "        model.fc = nn.Linear(128 * 7 * 7, num_classes)\n",
        "        outputs = model(images)\n",
        "        if outputs.size(0) != len(target_indices):\n",
        "            continue  # Skip batch if sizes do not match\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(outputs, torch.tensor(target_indices))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Total Training Time: {training_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "sZF8pAIEHUX4",
        "outputId": "8e2cfe29-f527-4c37-c029-ed0dda5bc6ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=14.39s)\n",
            "creating index...\n",
            "index created!\n",
            "Epoch 1/3, Loss: 2.627627372741699\n",
            "Epoch 2/3, Loss: 2.7098464965820312\n",
            "Epoch 3/3, Loss: 2.6445679664611816\n",
            "Total Training Time: 695.14 seconds\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}